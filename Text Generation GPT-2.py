# -*- coding: utf-8 -*-
"""TextGenGPT2INTERproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocOfj4AQCey37yF-iiNAd8LC2XUmWnJG
"""

! pip install transformers

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "gpt2-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)





import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

input_text = "i am really impressed"
max_length = 128

# Corrected code
input_ids = tokenizer(input_text, return_tensors="pt", max_length=max_length, truncation=True)

print(input_ids)

input_ids = input_ids["input_ids"].to(device)

output = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)

print(tokenizer.decode(output[0]))

output = model.generate(input_ids, max_length=max_length, num_beams=5,
                        do_sample=False, no_repeat_ngram_size=2)
tokenizer.decode(output[0])

# Set the pad token to eos token
tokenizer.pad_token = tokenizer.eos_token

input_text = "l like this"
max_length = 128

# Tokenize the input text and get the attention mask
inputs = tokenizer(input_text, return_tensors="pt", max_length=max_length, truncation=True, padding=True)

# Move input tensors to the same device as the model
device = "cuda" if torch.cuda.is_available() else "cpu"
inputs = {key: value.to(device) for key, value in inputs.items()}

# Move the model to the device
model.to(device)

# Generate the output using nucleus sampling
output = model.generate(
    input_ids=inputs['input_ids'],
    attention_mask=inputs['attention_mask'],
    max_length=max_length,
    do_sample=True,
    top_p=0.9,  # Correct parameter name for nucleus sampling
    pad_token_id=tokenizer.eos_token_id  # Explicitly set pad_token_id to eos_token_id
)
tokenizer.decode(output[0])